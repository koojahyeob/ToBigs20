# 8ì£¼ì°¨ Vision adv ë…¼ë¬¸ ë¦¬ë·° ê³¼ì œ

# VIT : An IMAGE IS WORTH 16x16 WORDS : TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

## 1. Introduction

- **Inductive Bias (ê·€ë‚©ì  í¸í–¥)**
    
    ì˜ë¯¸ : trainingì—ì„œ ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì ì ˆí•œ ê·€ë‚©ì  ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì´ ê°€ì§€ê³  ìˆëŠ” ê°€ì •ë“¤ì˜ ì§‘í•©
    
    DNNì˜ ê¸°ë³¸ì ì¸ ìš”ì†Œë“¤ì˜ inductive bias
    
    1. FC (fully connected) : ì…ë ¥ ë° ì¶œë ¥ elementê°€ ëª¨ë‘ ì—°ê²°ë˜ì–´ ìˆì–´ êµ¬ì¡°ì ìœ¼ë¡œ íŠ¹ë³„í•œ relational inductive bias ê°€ì • x
    2. Convolutional NN :  ì‘ì€ í¬ê¸°ì˜ ì»¤ë„ë¡œ ì´ë¯¸ì§€ë¥¼ ì§€ì—­ì ìœ¼ë¡œ ë³´ë©°, ë™ì¼í•œ ì»¤ë„ë¡œ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë³¸ë‹¤ëŠ” ì ì—ì„œ localityì™€ transitional invariance(ì…ë ¥ì˜ ìœ„ì¹˜ê°€ ë³€í•´ë„ ì¶œë ¥ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.) íŠ¹ì„± ì§€ë‹˜
    3. Recurrent  NN : ì…ë ¥í•œ ë°ì´í„°ë“¤ì´ ì‹œê°„ì  íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ sequentialityì™€ temporal invariance íŠ¹ì„± ì§€ë‹˜
    
    <aside>
    ğŸ’¡ **TransformerëŠ” self attention ê¸°ë°˜ìœ¼ë¡œ, CNN ë° RNN ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ inductive biasê°€ ë‚® ì¶©ë¶„í•˜ì§€ ëª»í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œ ì¼ë°˜í™”ê°€ ì˜ ë˜ì§€  ì•ŠëŠ”ë‹¤.**
    
    </aside>
    

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled.png)

- **Vision Transformer (VIT) ê°œìš”**
    - ë³¸ ì—°êµ¬ì—ì„œ NLPì—ì„œ ì‚¬ìš©ë˜ëŠ” self-attention ê¸°ë°˜ ì•„í‚¤í…ì²˜ì¸ Transformerê°€ ì£¼ìš” ëª¨ë¸ë¥¼ ì´ë¯¸ì§€ì— ê·¸ëŒ€ë¡œ ì ìš©ì‹œí‚¨ Vision Transformer (VIT) ì œì•ˆ
    - VIT â†’ ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í•  í•˜ëŠ”ë°, ì´ëŠ” NLPì˜ ë‹¨ì–´ì™€ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë¶„í• í•œ ê° íŒ¨ì¹˜ì˜ linear embedding sequenceë¥¼ Transformerì˜ input ê°’ìœ¼ë¡œ ë„£ì–´ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•œë‹¤.
    - ì¤‘ê°„ ê·œëª¨ì˜ ImageNetê³¼ ê°™ì€ datasetì—ì„œ trainingëœ ëª¨ë¸ì€ ê°•í•œ ì •ê·œí™” ì—†ì´ë„ ë¹„ìŠ·í•œ í¬ê¸°ì˜ ResNetë³´ë‹¤ ì•½ê°„ ë‚®ì€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. â†’ VITê°€ ì¼ë¶€ CNNë³´ë‹¤ inductive biasê°€ ë‚®ì•„ ë°ì´í„° ì–‘ì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œëŠ” ì˜ ì¼ë°˜í™”ê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
    - í•˜ì§€ë§Œ ë” í° dataset ì´ë¯¸ì§€ë¡œ í›ˆë ¨ì‹œí‚¨ë‹¤ë©´,  ì¶©ë¶„í•œ ê·œëª¨ë¡œ ì‚¬ì „ í›ˆë ¨ì´ ì§„í–‰ë˜ê³ , ë°ì´í„° í¬ì¸íŠ¸ê°€ ì ì€ ì‘ì—…ìœ¼ë¡œ ì „ì´ë  ë•Œ í›Œë¥­í•œ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.
    - ImageNet-21k ë°ì´í„°ì…‹ì´ë‚˜ JFT-300M ë°ì´í„°ì…‹ì—ì„œ VITê°€ SOTA ì„±ëŠ¥ì„ ë„ì¶œí•˜ëŠ” ê²ƒì„ í†µí•´ large scale í•™ìŠµì´ ë‚®ì€ inductive biasë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ í•´ì†Œì‹œí‚¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

## 2. Related Work and Proposed METHOD

- **VIT ëª¨ë¸ êµ¬ì¡° - ì´ 5 Stepìœ¼ë¡œ ì§„í–‰**

**>> Step 1** :  ì´ë¯¸ì§€ x ì— ëŒ€í•´ì„œ (PxP) í¬ê¸°ì˜ íŒ¨ì¹˜ Nê°œë¡œ ë¶„í• í•˜ì—¬ íŒ¨ì¹˜ sequence $x_p$ êµ¬ì¶•

**>> Step 2** : Trainable linear projectionì„ í†µí•´ $x_p$ì˜ ê° íŒ¨ì¹˜ë¥¼ flattení•œ ë²¡í„°ë¥¼ D ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì´ë¥¼ patch embeddingìœ¼ë¡œ ì‚¬ìš© (í•˜ë‚˜ í•˜ë‚˜ì˜ patch â†’ NLPì˜ tokenìœ¼ë¡œ ê°„ì£¼)

*learnable positional embeddingê³¼ element-wise sumìœ¼ë¡œ ê²°í•©ëœ embeddingì„ ì‚¬ìš©í•˜ì—¬ 0ë²ˆì§¸ patch + position embeddingì—ëŠ” classì„ ë¶€ì—¬ â†’ patch embedding*

**>> Step 3** : learnable class embeddingê³¼ patch embeddingì— learnable position embeddingì„ ë”í•œë‹¤,

**>> Step 4** : ì„ë² ë”© â†’ Vanilla Transformer encoderì— input ê°’ìœ¼ë¡œ ë„£ì–´ ë§ˆì§€ë§‰ layerì—ì„œ class embeddingì— ëŒ€í•œ outputì¸, image representation ë„ì¶œ (BERTì—ì„œ ë™ì¼í•˜ê²Œ, class tokenì„ ë„£ê³  ê° tokenì— ëŒ€í•œ representation ì‚¬ìš©)

**>> Step 5** : MLP Headì— image representationì„ input ìœ¼ë¡œ ë„£ì–´ ìµœì¢… ì´ë¯¸ì§€ì˜ class ë¶„ë¥˜

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%201.png)

- **Positional Embedding**
    
    ViTì—ì„œëŠ” ì•„ë˜ 4ê°€ì§€ position embedingì„ ì‹œë„í•œ í›„, ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ íš¨ê³¼ê°€ ì¢‹ì€ 1D position embeddingì„ ViTì— ì‚¬ìš©í•œë‹¤.
    
    1. No positional information : Considering the inputs as a bag of patches.
    2. 1-dimenstional positional embedding : Considering the inputs as a sequence of patches in the raster order(ë°°ì—´ëœ ìˆœì„œ).
    3. 2-dimenstional positional embedding : Considering the inputs as a grid of patches in two dimensions(ê³µê°„ì  ë°°ì—´ ì²´ê³„ì ìœ¼ë¡œ ì´í•´).
    4. Relative positional embeddings : Considering the relative dinstance between patches to encode the spatial information as intead of their absolute position.
    
    ![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%202.png)
    
- **Transformer Encoder**
    - ViT â†’ Multi-head Self Attention (MSA)ì™€ MLP blockìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.
    - MLPëŠ” 2ê°œì˜ layerë¥¼ ê°€ì§€ë©°, GELU activation functionì„ ì‚¬ìš©í•œë‹¤.
        
        ![ì¶œì²˜ : GAUSSIAN ERROR LINEAR UNITS (GELUS)](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%203.png)
        
        ì¶œì²˜ : GAUSSIAN ERROR LINEAR UNITS (GELUS)
        
    - ê° blockì˜ ì•ì—ëŠ” Layer Norm (LN)ì„ ì ìš©í•˜ê³ , ê° blockì˜ ë’¤ì—ëŠ” residual connection (+ ë¶€ë¶„ í•´ë‹¹)ì„ ì ìš©í•œë‹¤.
        
        ![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%204.png)
        

ìˆ˜ì‹

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%205.png)

- **ViT ê´€ì ì—ì„œì˜ Inducitve Bias**
    - ViTì—ì„œ MLPëŠ” localityì™€ translation equivariance(ì…ë ¥ì´ ë¹„ë€œì— ë”°ë¼ ì¶œë ¥ë„ ë°”ë€ë‹¤.)ê°€ ìˆì§€ë§Œ, MSAëŠ” globalí•˜ê¸° ë•Œë¬¸ì— CNNë³´ë‹¤ image - specific inductive biasê°€ ë‚®ë‹¤.
    - ë– ë¼ì„œ ì•„ë˜ ë‘ ê°€ì§€ ë°©ë²•ì„ í†µí•´ ViTì— inductive biasì˜ ì£¼ì…ì„ ì‹œë„í•œë‹¤.
    1. Patch extraction : cutting the image into patches (íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¥¼ ìë¥¸ë‹¤.
    2. Resolution adjustment(í•´ìƒë„ ì¡°ì •) :  adjusting the position embeddings for images of different resolution at fine-tuning.
- **Hybrid Architecture**
    - ViT â†’ raw imageê°€ ì•„ë‹Œ CNNìœ¼ë¡œ ì¶”ì¶œí•œ raw imageì˜ feature mapì„ í™œìš©í•  ìˆ˜ ìˆë‹¤.
    - feature mapì€ ì´ë¯¸ raw imageì˜ ê³µê°„ì  ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ hybrid architectureëŠ” íŒ¨ì¹˜ í¬ê¸°ë¥¼ 1x1ë¡œ ì„¤ì •í•´ë„ ë¬´ë°©í•˜ë‹¤.
    - 1x1 í¬ê¸°ì˜ patchë¥¼ ì‚¬ìš©í•  ê²½ìš° feature ampì˜ ê³µê°„ ì°¨ì›ì„ flattení•˜ì—¬ ê° vectorì— linear projectionì„ ì ìš©í•˜ë©´ ëœë‹¤.
- **Fine-tuning and Higher Resolution**
    - Large scaleë¡œ ViTë¥¼ ì‚¬ì „ í›ˆë ¨í•œ í›„, í•´ë‹¹ ëª¨ë¸ì„ downstream taskì— fine-tuningí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    - ViTë¥¼ fine-tuning í•  ë•Œ, ViTì˜ pre-trained prediction headë¥¼ zero-initialized feedforward layerë¡œ ëŒ€ì²´í•œë‹¤.
    - ë˜í•œ ViTë¥¼ fine-tuning í•  ë•Œ, pre-trainingê³¼ ë™ì¼í•œ íŒ¨ì¹˜ì˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¡œ fine-tuningì„ í•˜ë©´ sequence ê¸¸ì´ê°€ ë” ê¸¸ì–´ì§„ë‹¤.
    - ViTëŠ” ê°€ë³€ì  ê¸¸ì´ì˜ patchë“¤ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, pre-trained position embeddingì€ ì˜ë¯¸ê°€ ì‚¬ë¼ì§€ë¯€ë¡œ pre-trained position embeddingì„ ì›ë³¸ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ì— ë”°ë¼ 2D interpolation(ë³´ê°„ë²•)í•˜ì—¬ ì‚¬ìš©í•œë‹¤.
    
    â†’ Fine- tuning ì‹œ MLP headë§Œ ë³€ê²½ì„ í•´ì„œ ëª¨ë¸ êµ¬ì¶•í•œë‹¤.
    

## 3. Experiments

- **Datasets**
    - ViTëŠ” ì•„ë˜ì™€ ê°™ì´ classì™€ ì´ë¯¸ì§€ì˜ ê°œìˆ˜ê°€ ê°ê° ë‹¤ë¥¸ 3ê°œì˜ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ pre-train ì§„í–‰í•œë‹¤.
    - ì•„ë˜ì˜ benchmark tasksë¥¼ downstream taskë¡œ í•˜ì—¬ pre-trained ViTì˜ representation ì„±ëŠ¥ì„ ê²€ì¦í•œë‹¤.
        - ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,
        2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008)
        - 19-task VTAB classification suite

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%206.png)

- **Model Variants**
    - ViTëŠ” ì•„ë˜ì™€ ê°™ì´ ì´ 3ê°œì˜ volumeì— ëŒ€í•´ ì‹¤í—˜ ì§„í–‰, ë‹¤ì–‘í•œ íŒ¨ì¹˜ í¬ê¸°ì— ëŒ€í•´ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.
    - Baseline CNNì€ batch normalization layerë¥¼ group normalizationìœ¼ë¡œ ë³€ê²½í•˜ê³  standarized convolutional layerë¥¼ ì‚¬ìš©í•˜ì—¬ transfer learningì— ì í•©í•œ Big Transformer (BiT) êµ¬ì¡°ì˜ ResNetì„ ì‚¬ìš©í•œë‹¤.

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%207.png)

- **Comparison to SOTA (State of the Art)**
    - ë³¸ ì‹¤í—˜ì—ì„œ 14x14 íŒ¨ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ViT-Hugeì™€ 16x16 íŒ¨ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•œ ViT-Largeì˜ ì„±ëŠ¥ì„ baselineê³¼ ë¹„êµí–ˆë‹¤.
    - JFT datasetì—ì„œ pre-trainingí•œ ViT-L/16 ëª¨ë¸ì´ ëª¨ë“  downstream taskì— ëŒ€í•˜ì—¬ BiT-Lë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë„ì¶œí•œë‹¤.
    - ViT-L/14 ëª¨ë¸ì€ ViT-L/16 ëª¨ë¸ë³´ë‹¤ í–¥ìƒëœ ì„±ëŠ¥ì„ ë„ì¶œí•˜ì˜€ìœ¼ë©°, BiT-L ëª¨ë¸ë³´ë‹¤ í•™ìŠµ ì‹œê°„ ë˜í•œ í›¨ì”¬ ì§§ì•˜ë‹¤.
        
        ![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%208.png)
        
    - 19- task VTAB classification suiteë¥¼ ì•„ë˜ì™€ ê°™ì´ 3ê°€ì§€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.
        - Natural : task like Pets, CIFAR, etc
        - Speicalized : Medical and satellite imagery
        - Structured : tasks that require geometric understanding like localization
    - ì „ì²´ ë°ì´í„°ë¿ë§Œ ì•„ë‹ˆë¼ ê° ê·¸ë£¹ì—ì„œë„ ViT-H/14ê°€ ì¢‹ì€ ê²°ê³¼ë¥¼ ë„ì¶œí–ˆë‹¤.
        
        ![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%209.png)
        

- **Pre-training Data Requirements**
    - ë³¸ ì‹¤í—˜ì—ì„œëŠ” pre-training ë°ì´í„°ì…‹ì˜ í¬ê¸°ì— ë”°ë¥¸ fine-tuning ì„±ëŠ¥ì„ í™•ì¸í–ˆë‹¤.
    
    ![ê° ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ pre-trainingí•œ ViTë¥¼ ImageNetì— transfer learningí•œ ì •í™•ë„ë¥¼ í™•ì¸í•œ ê²°ê³¼, ë°ì´í„°ê°€ í´ìˆ˜ë¡ ViTê°€ BiTë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ê³  í¬ê¸°ê°€ í° ViT ëª¨ë¸ì´ íš¨ê³¼ê°€ ìˆë‹¤.](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2010.png)
    
    ê° ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ pre-trainingí•œ ViTë¥¼ ImageNetì— transfer learningí•œ ì •í™•ë„ë¥¼ í™•ì¸í•œ ê²°ê³¼, ë°ì´í„°ê°€ í´ìˆ˜ë¡ ViTê°€ BiTë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ê³  í¬ê¸°ê°€ í° ViT ëª¨ë¸ì´ íš¨ê³¼ê°€ ìˆë‹¤.
    
    ![JFTë¥¼ ê°ê° ë‹¤ë¥¸ í¬ê¸°ë¡œ ëœë¤ ìƒ˜í”Œë§í•œ datasetì„ í™œìš©í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼, ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ CNNì˜ inductive biasê°€ íš¨ê³¼ê°€ ìˆì—ˆìœ¼ë‚˜, í° ë°ì´í„°ì…‹ì—ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¶©ë¶„í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2011.png)
    
    JFTë¥¼ ê°ê° ë‹¤ë¥¸ í¬ê¸°ë¡œ ëœë¤ ìƒ˜í”Œë§í•œ datasetì„ í™œìš©í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼, ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ CNNì˜ inductive biasê°€ íš¨ê³¼ê°€ ìˆì—ˆìœ¼ë‚˜, í° ë°ì´í„°ì…‹ì—ì„œëŠ” ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¶©ë¶„í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.
    

<aside>
ğŸ’¡ **CNN, ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” inductive bias ë•ë¶„ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆì§€ë§Œ, ë§ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ViTê°€ í›¨ì”¬ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆë‹¤.**

</aside>

- **Scaling Study**
    - JFTë¥¼ ê¸°ë°˜ìœ¼ë¡œ pre-training cost ëŒ€ë¹„ transfer ì„±ëŠ¥ì„ ê²€ì¦í•˜ì—¬ ëª¨ë¸ë“¤ì˜ scaling studyë¥¼ ì§„í–‰í–ˆë‹¤.
        - Pre-training cost : TPUv3 acceleratorì—ì„œ ëª¨ë¸ì˜ inference ì†ë„ ê´€ë ¨ ì§€í‘œ ì˜ë¯¸
    - ViTê°€ ì„±ëŠ¥, costì˜ trade-offì—ì„œ ResNet (BiT)ë³´ë‹¤ ìš°ì„¸í•œ ê²ƒì„ ê²€ì¦í–ˆë‹¤.
    - Costê°€ ì¦ê°€í•  ìˆ˜ë¡ Hybridì™€ ViTì˜ ì„±ëŠ¥ê³¼ costì˜ trade-off ì°¨ì´ê°€ ê°ì†Œí•œë‹¤.

![Untitled](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2012.png)

- Inspecting Vision Transformer
    - ë³¸ ì‹¤í—˜ì—ì„œëŠ” ViTê°€ ì–´ë–»ê²Œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•œ ì‹¤í—˜ ì§„í–‰

![flatten íŒ¨ì¹˜ë¥¼ patch embeddingìœ¼ë¡œ ë³€í™˜í•˜ëŠ” linear projectionì˜ principal componentsë¥¼ ë¶„ì„í–ˆë‹¤.](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2013.png)

flatten íŒ¨ì¹˜ë¥¼ patch embeddingìœ¼ë¡œ ë³€í™˜í•˜ëŠ” linear projectionì˜ principal componentsë¥¼ ë¶„ì„í–ˆë‹¤.

![íŒ¨ì¹˜ ê°„ position embeddingì˜ ìœ ì‚¬ë„ë¥¼ í†µí•´ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìˆëŠ” íŒ¨ì¹˜ë“¤ì˜ position embeddingì´ ìœ ì‚¬í•œì§€ í™•ì¸í–ˆë‹¤.](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2014.png)

íŒ¨ì¹˜ ê°„ position embeddingì˜ ìœ ì‚¬ë„ë¥¼ í†µí•´ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìˆëŠ” íŒ¨ì¹˜ë“¤ì˜ position embeddingì´ ìœ ì‚¬í•œì§€ í™•ì¸í–ˆë‹¤.

![ViTì˜ layerë³„ í‰ê·  attention dinstanceë¥¼ í™•ì¸í•œ ê²°ê³¼, ì´ˆë°˜ layerì—ì„œë„ attentionì„ í†µí•´ ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš©í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.](8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Vision%20adv%20%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%20%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%2077f60f2d06d44775992c0385ffaa069b/Untitled%2015.png)

ViTì˜ layerë³„ í‰ê·  attention dinstanceë¥¼ í™•ì¸í•œ ê²°ê³¼, ì´ˆë°˜ layerì—ì„œë„ attentionì„ í†µí•´ ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš©í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì°¸ê³ ìë£Œ

[https://www.youtube.com/watch?v=0kgDve_vC1o&t=57s&pp=ygUEdml0IA%3D%3D](https://www.youtube.com/watch?v=0kgDve_vC1o&t=57s&pp=ygUEdml0IA%3D%3D)

inductive bias 

[https://velog.io/@euisuk-chung/Inductive-Biasë€](https://velog.io/@euisuk-chung/Inductive-Bias%EB%9E%80)

[https://moon-walker.medium.com/transformerëŠ”-inductive-biasì´-ë¶€ì¡±í•˜ë‹¤ë¼ëŠ”-ì˜ë¯¸ëŠ”-ë¬´ì—‡ì¼ê¹Œ-4f6005d32558](https://moon-walker.medium.com/transformer%EB%8A%94-inductive-bias%EC%9D%B4-%EB%B6%80%EC%A1%B1%ED%95%98%EB%8B%A4%EB%9D%BC%EB%8A%94-%EC%9D%98%EB%AF%B8%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C-4f6005d32558)